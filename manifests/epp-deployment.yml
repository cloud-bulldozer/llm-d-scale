apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-d-scale-epp
  namespace: llm-d-scale
spec:
  replicas: 1
  selector:
    matchLabels:
      inferencepool: llm-d-scale-epp
  template:
    metadata:
      labels:
        inferencepool: llm-d-scale-epp
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/infra
                operator: DoesNotExist
              - key: node-role.kubernetes.io/gpu
                operator: Exists
      tolerations:
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
      serviceAccountName: llm-d-scale-epp
      # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
      terminationGracePeriodSeconds: 130
      containers:
      - name: epp
        image: {{ .image }}
        imagePullPolicy: IfNotPresent
        args:
        - -poolName
        - llm-d-scale
        - -poolNamespace
        - llm-d-scale
        - --v
        - "4"
        - --grpcPort
        - "9002"
        - -grpcHealthPort
        - "9003"
        - -metricsPort
        - "9090"
        - -configFile
        - "{{ .configFile }}"
        # https://pkg.go.dev/flag#hdr-Command_line_flag_syntax; space is only for non-bool flags
        - "--enablePprof=true"
        - "--modelServerMetricsPath=/metrics"
        - "--modelServerMetricsScheme=http"
        - "--modelServerMetricsHttpsInsecureSkipVerify=true"
        ports:
        - name: grpc
          containerPort: 9002
        - name: grpc-health
          containerPort: 9003
        - name: metrics
          containerPort: 9090
        - containerPort: 5557
          name: zmq
          protocol: TCP
        livenessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        readinessProbe:
          grpc:
            port: 9003
            service: inference-extension
          initialDelaySeconds: 5
          periodSeconds: 10
        env:
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              key: HF_TOKEN
              name: llm-d-hf-token
        volumeMounts:
        - name: plugins-config-volume
          mountPath: "/config"
      volumes:
      - name: plugins-config-volume
        configMap:
          name: llm-d-scale-epp