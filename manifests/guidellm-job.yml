apiVersion: batch/v1
kind: Job
metadata:
  generateName: guidellm-{{.JobName}}-
  namespace: llm-d-scale
spec:
  completionMode: NonIndexed
  completions: {{ .completions }}
  parallelism: {{ .parallelism}}
  completionMode: Indexed
  backoffLimit: 0
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/infra
                operator: DoesNotExist
              - key: node-role.kubernetes.io/worker
                operator: Exists
      restartPolicy: Never
      containers:
      - name: guidellm
        volumeMounts:
        - name: results
          mountPath: /results
        env:
        {{ range $k, $v := .env }}
          - name: {{ $k }}
            value: "{{ $v }}"
        {{ end }}
          - name: HF_HOME # Required to avoid permissions issues when downloading the tokenizer and creating the cache directory
            value: /results
        image: {{ .image }}
        command: ["/bin/sh", "-c"]
        args:  # TODO: optimize model downloading
        - |
          guidellm benchmark run &&                      
          guidellm_parser.py \
          --results=benchmarks.json \
          --uuid={{ .UUID }} \
          --es-server={{ .ES_SERVER }} \
          --es-index={{ .ES_INDEX }} \
          --job-name={{ .JobName }} \
          --sample=${JOB_COMPLETION_INDEX} &&
          sleep {{ .pause }}
      volumes:
      - name: results
        emptyDir: {}

